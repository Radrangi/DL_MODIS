{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn as rnn_cell\n",
    "import numpy as np\n",
    "import io\n",
    "from util.tf_utils import tf_confusion_metrics\n",
    "import inspect\n",
    "import util.eval as eval\n",
    "import util.plots as plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \"\"\"\n",
    "    Tensorflow Graph using Recurrent LSTM layers and fully connected softmax layer for field identification\n",
    "    with multispectral/temporal data acquired from satellite imagery\n",
    "    Params\n",
    "        tf placeholders:\n",
    "            X           Input data cube of dimensions [batch_size x max_observations x n_input]\n",
    "            y           Target data Tensor of dimensions [batch_size x max_observations]\n",
    "            seq_lenghts Number of observations for each batch if observation < max_obs data is\n",
    "                        padded with zeros [batch_size]\n",
    "        input parameters:\n",
    "            n_input     length of observed pixel values. [n_pixels * n_bands + n_time]\n",
    "                n_pixels    number of observed pixels (default 3*3)\n",
    "                n_bands     number of observed bands  (default 6)\n",
    "                n_time      number of time parameters (default 1 e.g. day of year)\n",
    "            n_classes   number of target classes\n",
    "            batch_size  number of batches\n",
    "            max_obs     maximum number of observations if seq_lengs < max_obs matrices will be padded\n",
    "                        controls number of iterations in rnn layers (aka sequence length)\n",
    "        network specific parameters\n",
    "            n_layers    number of rnn layers (aka depth)\n",
    "            learning_rate\n",
    "            dropout_keep_prob\n",
    "            logdir\n",
    "    Marc.Russwurm@tum.de\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_input=9 * 6 + 1, n_classes=20, batch_size=50, max_obs=26,\n",
    "                 n_layers=2, dropout_keep_prob=.5, adam_lr=1e-3, adam_b1=0.9, adam_b2=0.999, adam_eps=1e-8,\n",
    "                 fc_w_stddev=0.1, fc_b_offset=0.1, n_cell_per_input=1,rnn_cell_type=\"basiclstm\", gpu=None):\n",
    "        # save input arguments\n",
    "        self.args = inspect.getargvalues(inspect.currentframe()).locals\n",
    "        del self.args[\"self\"]  # delete self argument\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        with tf.device(None):\n",
    "\n",
    "            with tf.variable_scope('input'):\n",
    "                # block of [batch_size x max_obs x n_input]\n",
    "                self.X = tf.placeholder(tf.float32, [batch_size, max_obs, n_input], name=\"X\")\n",
    "                self.y_ = self.y = y_ = tf.placeholder(tf.float32, [batch_size, max_obs, n_classes], name=\"y\")\n",
    "                self.seq_lengths = seq_lengths = tf.placeholder(tf.int32, [batch_size], name=\"seq_lengths\")\n",
    "                #self.y = y = tf.reshape(self.y_, [-1, n_classes], name=\"y\")\n",
    "\n",
    "            with tf.name_scope('RNN'):\n",
    "                self.n_rnn_cells = n_rnn_cells = n_cell_per_input * n_input\n",
    "\n",
    "                if rnn_cell_type == \"basiclstm\":\n",
    "                    cell = rnn_cell.BasicLSTMCell(n_rnn_cells)\n",
    "                if rnn_cell_type == \"lstm\":\n",
    "                    cell = rnn_cell.LSTMCell(n_rnn_cells)\n",
    "                if rnn_cell_type == \"lstm_peephole\":\n",
    "                    cell = rnn_cell.LSTMCell(n_rnn_cells, use_peepholes=True)\n",
    "                elif rnn_cell_type == \"gru\":\n",
    "                    cell = rnn_cell.BasicLSTMCell(n_rnn_cells)\n",
    "                elif rnn_cell_type == \"rnn\":\n",
    "                    cell = rnn_cell.BasicRNNCell(n_rnn_cells)\n",
    "\n",
    "                # dropout Wrapper\n",
    "                cell = tf.contrib.rnn.DropoutWrapper(cell=cell, output_keep_prob=dropout_keep_prob)\n",
    "                self.cell = cell = rnn_cell.MultiRNNCell([cell] * n_layers)\n",
    "\n",
    "                # tensor with class labels of dimension [batch_size x max_obs]\n",
    "                # defined as Variable to carry values to next iteration (not trainable must be declared explicitly)\n",
    "                self.state = state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "                # rnn_outputs: block of [batch_size x max_obs x rnn_size]\n",
    "                # data is padded with zeros after seq_length\n",
    "                outputs, last_states = tf.nn.dynamic_rnn(cell, self.X, initial_state=state, sequence_length=seq_lengths,\n",
    "                                                         time_major=False)\n",
    "\n",
    "                self.outputs = outputs\n",
    "                self.last_states = last_states\n",
    "\n",
    "            with tf.name_scope('fc'):\n",
    "                # reshape outputs to: block of [batch_size * max_obs x rnn_size]\n",
    "                softmax_in = tf.reshape(outputs, [-1, n_rnn_cells])\n",
    "                softmax_w = tf.Variable(tf.truncated_normal([n_rnn_cells, n_classes], stddev=fc_w_stddev), name=\"W_softmax\")\n",
    "                softmax_b = tf.Variable(tf.constant(fc_b_offset, shape=[n_classes]), name=\"b_softmax\")\n",
    "\n",
    "                softmax_out = tf.matmul(softmax_in, softmax_w) + softmax_b\n",
    "                self.logits = logits = tf.reshape(softmax_out, [batch_size, -1, n_classes])\n",
    "\n",
    "            with tf.name_scope('train'):\n",
    "                # Define loss and optimizer\n",
    "\n",
    "                # create mask for cross entropies incases where seq_lengths < max_max_obs\n",
    "                # masking from http://stackoverflow.com/questions/34128104/tensorflow-creating-mask-of-varied-lengths\n",
    "\n",
    "                with tf.name_scope('mask'):\n",
    "                    lengths_transposed = tf.expand_dims(seq_lengths, 1)\n",
    "\n",
    "                    range = tf.range(0, max_obs, 1)\n",
    "                    range_row = tf.expand_dims(range, 0)\n",
    "\n",
    "                    self.mask = mask = tf.less(range_row, lengths_transposed)\n",
    "\n",
    "                self.cross_entropy_matrix = cross_entropy_matrix = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_)\n",
    "                self.masked_cross_entropy_matrix = masked_cross_entropy_matrix = tf.where(mask, cross_entropy_matrix,\n",
    "                                                                                           tf.zeros(mask.get_shape()))\n",
    "                self.cross_entropy_matrix = cross_entropy_matrix = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_)\n",
    "\n",
    "                # normalize with total number of observations\n",
    "                self.cross_entropy = cross_entropy = tf.reduce_sum(cross_entropy_matrix) / tf.cast(\n",
    "                    tf.reduce_sum(seq_lengths), tf.float32)\n",
    "                tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "                # grad_train_op = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "                self.train_op = tf.train.AdamOptimizer(learning_rate=adam_lr, beta1=adam_b1, beta2=adam_b2,\n",
    "                                                       epsilon=adam_eps).minimize(cross_entropy)\n",
    "                # tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "            with tf.name_scope('evaluation'):\n",
    "\n",
    "                self.probabilities = probs = tf.nn.softmax(logits, name=\"full_probability_matrix\")\n",
    "\n",
    "                # Evaluate model\n",
    "                predicted = tf.argmax(logits, 2)\n",
    "                targets = tf.argmax(y_, 2)\n",
    "\n",
    "                correct_pred = tf.equal(predicted, targets)\n",
    "                masked_correct_pred = tf.logical_and(mask, correct_pred)\n",
    "                self.accuracy_op = accuracy = tf.reduce_sum(tf.cast(masked_correct_pred, tf.float32)) / tf.cast(\n",
    "                    tf.reduce_sum(seq_lengths), tf.float32)\n",
    "                tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "                self.probs_list = probs_list = tf.reshape(probs, (-1, n_classes))\n",
    "                predicted_list = tf.reshape(predicted, [-1])\n",
    "                targets_list = tf.reshape(targets, [-1])\n",
    "\n",
    "                mask_list = tf.reshape(mask, [-1])\n",
    "\n",
    "                one_hot_targets = tf.one_hot(targets_list, n_classes)\n",
    "                scores = tf.boolean_mask(probs_list, tf.cast(one_hot_targets, tf.bool))\n",
    "\n",
    "                # mask of individual number of observations\n",
    "                obs_list = tf.tile(tf.range(0, max_obs), [batch_size])\n",
    "                obs_matrix = tf.matmul(tf.expand_dims(obs_list, 1), tf.ones([1, n_classes], dtype=tf.int32))\n",
    "\n",
    "\n",
    "\n",
    "                probs_matrix_mask = probs_matrix_mask = tf.transpose(tf.reshape(tf.tile(mask_list, [n_classes]),[n_classes,-1]))\n",
    "\n",
    "                self.scores = tf.boolean_mask(probs_list, probs_matrix_mask)\n",
    "                self.targets = tf.boolean_mask(tf.reshape(y_, [-1,n_classes]), probs_matrix_mask)\n",
    "                self.obs = tf.boolean_mask(obs_list, mask_list)\n",
    "\n",
    "                # drop all values which are > seqlength\n",
    "                #self.scores = tf.boolean_mask(scores, mask_list)\n",
    "                #self.targets = tf.boolean_mask(targets_list, mask_list)\n",
    "                #self.obs = tf.boolean_mask(obs_list, mask_list)\n",
    "\n",
    "                self.confusion_matrix = confusion_matrix = tf.contrib.metrics.confusion_matrix(\n",
    "                    tf.boolean_mask(targets_list, mask_list),\n",
    "                    tf.boolean_mask(predicted_list, mask_list),\n",
    "                    num_classes=n_classes)\n",
    "\n",
    "                confusion_matrix = tf.cast(confusion_matrix, tf.uint8)\n",
    "                confusion_matrix = tf.expand_dims(confusion_matrix, 2)\n",
    "                confusion_matrix = tf.expand_dims(confusion_matrix, 0)\n",
    "                tf.summary.image(\"confusion_matrix\", confusion_matrix, max_outputs=3)\n",
    "\n",
    "                logits_ = tf.cast(logits, tf.uint8)\n",
    "                logits_ = tf.expand_dims(logits_, 3)\n",
    "                tf.summary.image(\"logits\", logits_, max_outputs=1)\n",
    "\n",
    "                probs_ = tf.cast(probs*255, tf.uint8)\n",
    "                probs_ = tf.expand_dims(probs_, 3)\n",
    "                tf.summary.image(\"probabilities\", probs_, max_outputs=1)\n",
    "\n",
    "                targets_ = tf.cast(y_, tf.uint8)\n",
    "                targets_ = tf.expand_dims(targets_, 3)\n",
    "                tf.summary.image(\"targets\", targets_, max_outputs=1)\n",
    "\n",
    "                # tf.add_to_collection(tf.GraphKeys.SUMMARIES, cm_im_summary)\n",
    "\n",
    "            # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "            self.merge_summary_op = tf.summary.merge_all()\n",
    "            self.init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unroll(x, y, seq_lengths):\n",
    "    \"\"\"\n",
    "        Reshapes and masks input and output data from\n",
    "        X(batchsize x n_max_obs x n_input) -> X_ (new_batchsize x n_input)\n",
    "        y(batchsize x n_max_obs x n_classes) -> X_ (new_batchsize x n_classes)\n",
    "        new_batch_size is variable representing batchsize * n_max_obs - invalid_observations\n",
    "        with invalid observations being observations > seq_length -> means\n",
    "        if at one point only 24 of maximum 26 images are available X is usually padded with zeros\n",
    "        this masking removes the last two observations\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # create mask for valid times of acquisition\n",
    "    batch_size, max_seqlengths, n_input = x.shape\n",
    "    np.arange(0, max_seqlengths) * np.ones((batch_size, max_seqlengths))\n",
    "    ones = np.ones([batch_size, max_seqlengths])\n",
    "    mask = np.arange(0, max_seqlengths) * ones < (seq_lengths * ones.T).T\n",
    "\n",
    "    new_x = x[mask]\n",
    "    new_y = y[mask]\n",
    "\n",
    "    return new_x, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNNModel:\n",
    "    \"\"\"\n",
    "    -- Copied from RNN TODO update to FC --\n",
    "    Tensorflow graph using ful\n",
    "    Tensorflow Graph using Fully connected layers and fully connected softmax layer for field identification\n",
    "    with multispectral/temporal data acquired from satellite imagery\n",
    "    Params\n",
    "        tf placeholders:\n",
    "            X           Input data cube of dimensions [batch_size x max_observations x n_input]\n",
    "            y           Target data Tensor of dimensions [batch_size x max_observations]\n",
    "            seq_lenghts Number of observations for each batch if observation < max_obs data is\n",
    "                        padded with zeros [batch_size]\n",
    "        input parameters:\n",
    "            n_input     length of observed pixel values. [n_pixels * n_bands + n_time]\n",
    "                n_pixels    number of observed pixels (default 3*3)\n",
    "                n_bands     number of observed bands  (default 6)\n",
    "                n_time      number of time parameters (default 1 e.g. day of year)\n",
    "            n_classes   number of target classes\n",
    "            batch_size  number of batches\n",
    "            max_obs     maximum number of observations if seq_lengs < max_obs matrices will be padded\n",
    "                        controls number of iterations in rnn layers (aka sequence length)\n",
    "        network specific parameters\n",
    "            n_layers    number of rnn layers (aka depth)\n",
    "            learning_rate\n",
    "            dropout_keep_prob\n",
    "            logdir\n",
    "    Marc.Russwurm@tum.de\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_input=9 * 6 + 1, n_classes=20, batch_size=50,\n",
    "                 n_layers=2, dropout_keep_prob=.5, adam_lr=1e-3, adam_b1=0.9, adam_b2=0.999, adam_eps=1e-8,\n",
    "                 fc_w_stddev=0.1, fc_b_offset=0.1, n_cell_per_input=1, activation_func=None, gpu=None):\n",
    "        # save input arguments\n",
    "        self.args = inspect.getargvalues(inspect.currentframe()).locals\n",
    "        del self.args[\"self\"]  # delete self\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        if activation_func is None:\n",
    "            activation_func = tf.nn.sigmoid\n",
    "        # alternative tf.nn.relu\n",
    "\n",
    "        # take\n",
    "        self.n_neurons = n_neurons = n_cell_per_input * n_input\n",
    "\n",
    "        with tf.device(None):\n",
    "\n",
    "            with tf.variable_scope('input'):\n",
    "                # block of [batch_size x max_obs x n_input]\n",
    "                self.X = X = tf.placeholder(tf.float32, [None, n_input], name=\"X\")\n",
    "                self.y = y = tf.placeholder(tf.float32, [None, n_classes], name=\"y\")\n",
    "                self.batch_size = batch_size = tf.placeholder(tf.int32, name=\"batch_size\")\n",
    "            with tf.name_scope('FC'):\n",
    "\n",
    "\n",
    "                # first fc layer: expand neuron dimensions from n_input to n_neurons\n",
    "                # list of fully connected weights matrices\n",
    "\n",
    "                fc_in = X\n",
    "\n",
    "                # first fc layer X:(batchsize x n_input) -> fc_in (batchsize x n_neurons)\n",
    "                fc_W0 = tf.Variable(tf.truncated_normal([n_input, n_neurons], stddev=fc_w_stddev), name=\"W0\")\n",
    "                fc_b0 = tf.Variable(tf.constant(fc_b_offset, shape=[n_neurons]), name=\"b0\")\n",
    "                h = activation_func(tf.matmul(fc_in, fc_W0) + fc_b0)\n",
    "\n",
    "                h = tf.nn.dropout(h, dropout_keep_prob)\n",
    "\n",
    "                # for all other fc layers\n",
    "                fc_W = []\n",
    "                fc_b = []\n",
    "                for i in range(1, n_layers):\n",
    "                    W = tf.Variable(tf.truncated_normal([n_neurons, n_neurons], stddev=fc_w_stddev), name=\"W\" + str(i))\n",
    "                    b = tf.Variable(tf.constant(fc_b_offset, shape=[n_neurons]), name=\"b\" + str(i))\n",
    "\n",
    "                    h = tf.matmul(h, W) + b\n",
    "\n",
    "                    # apply activation function\n",
    "                    h = activation_func(h)\n",
    "\n",
    "                    h = tf.nn.dropout(h, dropout_keep_prob)\n",
    "\n",
    "                fc_out = h\n",
    "\n",
    "            with tf.name_scope('fc_softmax'):\n",
    "                # reshape outputs to: block of [batch_size * max_obs x rnn_size]\n",
    "                softmax_in = tf.reshape(fc_out, [-1, n_neurons])\n",
    "                softmax_w = tf.Variable(tf.truncated_normal([n_neurons, n_classes], stddev=fc_w_stddev), name=\"W_softmax\")\n",
    "                softmax_b = tf.Variable(tf.constant(fc_b_offset, shape=[n_classes]), name=\"b_softmax\")\n",
    "\n",
    "                self.logits = logits = tf.matmul(softmax_in, softmax_w) + softmax_b\n",
    "\n",
    "            with tf.name_scope('train'):\n",
    "                # Define loss and optimizer\n",
    "\n",
    "                # create mask for cross entropies incases where seq_lengths < max_max_obs\n",
    "                # masking from http://stackoverflow.com/questions/34128104/tensorflow-creating-mask-of-varied-lengths\n",
    "\n",
    "                \"\"\" no masking needed\n",
    "                with tf.name_scope('mask'):\n",
    "                    lengths_transposed = tf.expand_dims(seq_lengths, 1)\n",
    "                    range = tf.range(0, max_obs, 1)\n",
    "                    range_row = tf.expand_dims(range, 0)\n",
    "                    self.mask = mask = tf.less(range_row, lengths_transposed)\n",
    "                \"\"\"\n",
    "\n",
    "                self.cross_entropy_matrix = cross_entropy_matrix = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "\n",
    "                # normalize with total number of observations\n",
    "                self.cross_entropy = cross_entropy = tf.reduce_sum(cross_entropy_matrix) / tf.cast(batch_size,\"float32\")\n",
    "\n",
    "                tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "                # grad_train_op = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "                self.train_op = tf.train.AdamOptimizer(learning_rate=adam_lr, beta1=adam_b1, beta2=adam_b2,\n",
    "                                                       epsilon=adam_eps).minimize(cross_entropy)\n",
    "                # tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "            with tf.name_scope('evaluation'):\n",
    "\n",
    "                self.probabilities = probs = tf.nn.softmax(logits, name=\"full_probability_matrix\")\n",
    "\n",
    "                # Evaluate model\n",
    "                predicted = tf.argmax(logits, 1)\n",
    "                targets = tf.argmax(y, 1)\n",
    "\n",
    "                correct_pred = tf.equal(predicted, targets)\n",
    "                self.accuracy_op = accuracy = tf.reduce_sum(tf.cast(correct_pred, tf.float32)) / tf.cast(batch_size, tf.float32)\n",
    "                tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "                self.probs_list = probs_list = tf.reshape(probs, (-1, n_classes))\n",
    "                predicted_list = tf.reshape(predicted, [-1])\n",
    "                targets_list = tf.reshape(targets, [-1])\n",
    "\n",
    "                one_hot_targets = tf.one_hot(targets_list, n_classes)\n",
    "                scores = tf.boolean_mask(probs_list, tf.cast(one_hot_targets, tf.bool))\n",
    "\n",
    "                self.scores = probs_list\n",
    "                self.targets = tf.reshape(y, [-1,n_classes])\n",
    "\n",
    "                # drop all values which are > seqlength\n",
    "                #self.scores = tf.boolean_mask(scores, mask_list)\n",
    "                #self.targets = tf.boolean_mask(targets_list, mask_list)\n",
    "                #self.obs = tf.boolean_mask(obs_list, mask_list)\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                self.confusion_matrix = confusion_matrix = tf.contrib.metrics.confusion_matrix(\n",
    "                    tf.boolean_mask(targets_list, mask_list),\n",
    "                    tf.boolean_mask(predicted_list, mask_list),\n",
    "                    num_classes=n_classes)\n",
    "                confusion_matrix = tf.cast(confusion_matrix, tf.uint8)\n",
    "                confusion_matrix = tf.expand_dims(confusion_matrix, 2)\n",
    "                confusion_matrix = tf.expand_dims(confusion_matrix, 0)\n",
    "                tf.summary.image(\"confusion matrix\", confusion_matrix, max_outputs=3)\n",
    "                logits_ = tf.cast(logits, tf.uint8)\n",
    "                logits_ = tf.expand_dims(logits_, 3)\n",
    "                tf.summary.image(\"logits\", logits_, max_outputs=1)\n",
    "                probs_ = tf.cast(probs*255, tf.uint8)\n",
    "                probs_ = tf.expand_dims(probs_, 3)\n",
    "                tf.summary.image(\"probabilities\", probs_, max_outputs=1)\n",
    "                targets_ = tf.cast(y_, tf.uint8)\n",
    "                targets_ = tf.expand_dims(targets_, 3)\n",
    "                tf.summary.image(\"targets\", targets_, max_outputs=1)\n",
    "                # tf.add_to_collection(tf.GraphKeys.SUMMARIES, cm_im_summary)\n",
    "                 \"\"\"\n",
    "\n",
    "            # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "            self.merge_summary_op = tf.summary.merge_all()\n",
    "            self.init_op = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unroll(x, y, seq_lengths):\n",
    "    \"\"\"\n",
    "        Reshapes and masks input and output data from\n",
    "        X(batchsize x n_max_obs x n_input) -> X_ (new_batchsize x n_input)\n",
    "        y(batchsize x n_max_obs x n_classes) -> X_ (new_batchsize x n_classes)\n",
    "        new_batch_size is variable representing batchsize * n_max_obs - invalid_observations\n",
    "        with invalid observations being observations > seq_length -> means\n",
    "        if at one point only 24 of maximum 26 images are available X is usually padded with zeros\n",
    "        this masking removes the last two observations\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # create mask for valid times of acquisition\n",
    "    batch_size, max_seqlengths, n_input = x.shape\n",
    "    np.arange(0, max_seqlengths) * np.ones((batch_size, max_seqlengths))\n",
    "    ones = np.ones([batch_size, max_seqlengths])\n",
    "    mask = np.arange(0, max_seqlengths) * ones < (seq_lengths * ones.T).T\n",
    "\n",
    "    new_x = x[mask]\n",
    "    new_y = y[mask]\n",
    "\n",
    "    return new_x, new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "n_input = 9 * 6 + 1\n",
    "n_classes = 2\n",
    "batch_size = 50\n",
    "max_obs = 26\n",
    "\n",
    "confusion_matrix = np.zeros((n_classes, n_classes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model = Model(n_input=n_input, n_classes=n_classes, n_layers=4, batch_size=batch_size, adam_lr=1e-3, dropout_keep_prob=0.5, n_cell_per_input=4, rnn_cell_type = \"lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = CNNModel(n_input=n_input, n_classes=n_classes, n_layers=2, batch_size=batch_size,\n",
    "                  adam_lr=1e-3, dropout_keep_prob=0.5, n_cell_per_input=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savedir = \"/home/acocac/post/tmp/lstm\"\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "\n",
    "init_from = None\n",
    "if init_from is not None:\n",
    "    args = pickle.load(open(os.path.join(init_from, \"args.pkl\"), \"rb\"))\n",
    "\n",
    "X = np.random.rand(batch_size, max_obs, n_input)\n",
    "y = np.random.rand(batch_size, max_obs, n_classes)\n",
    "\n",
    "seq_length = np.random.randint(16, max_obs, batch_size)\n",
    "\n",
    "summaryWriter = tf.summary.FileWriter(\"/home/acocac/post/tmp/log\", graph=tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([model.init_op])\n",
    "\n",
    "    feed = {model.X: X, model.y_: y, model.seq_lengths: seq_length}\n",
    "    # training step\n",
    "    for i in range(1, 30):\n",
    "        train_op, cross_entropy, new_confusion_matrix = \\\n",
    "            sess.run([model.train_op,\n",
    "                      model.cross_entropy,\n",
    "                      model.confusion_matrix], feed_dict=feed)\n",
    "\n",
    "        confusion_matrix += new_confusion_matrix\n",
    "        print(cross_entropy)\n",
    "\n",
    "        #a,b = eval.class_evaluation(confusion_matrix)\n",
    "        a,b = eval.calculate_accuracy_metrics(confusion_matrix)\n",
    "        scores, targets = sess.run([model.scores, tf.reshape(model.targets, [-1])], feed_dict=feed)\n",
    "        #fpr, tpr, threshold = roc_curve(targets, scores, 0)\n",
    "        #fpr, tpr, threshold = roc_curve(targets, scores, 0)\n",
    "\n",
    "        summary = sess.run(model.merge_summary_op, feed_dict=feed)\n",
    "        summaryWriter.add_summary(summary, i)\n",
    "\n",
    "    #buf = plots.plot_confusion_matrix(confusion_matrix, range(1, n_classes))\n",
    "    #image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    #image = tf.expand_dims(image, 0)\n",
    "    #summary_op = tf.image_summary(\"matplotlib conf matrix\", image)\n",
    "    #summary = sess.run(summary_op)\n",
    "    # summaryWriter.add_summary(summary, i)\n",
    "\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savedir = \"/home/acocac/post/tmp/cnn\"\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "\n",
    "# dump pickle args for loading\n",
    "#pickle.dump(model.args, open(os.path.join(savedir, \"args.pkl\"), \"wb\"))\n",
    "# dump human readable args\n",
    "#open(os.path.join(savedir, \"args.txt\"), \"w\").write(str(model.args))\n",
    "\n",
    "init_from = None\n",
    "if init_from is not None:\n",
    "    args = pickle.load(open(os.path.join(init_from, \"args.pkl\"), \"rb\"))\n",
    "\n",
    "\n",
    "X = np.random.rand(batch_size, max_obs, n_input)\n",
    "y = np.random.rand(batch_size, max_obs, n_classes)\n",
    "\n",
    "seq_length = np.random.randint(16, max_obs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (50, 26, 55) for Tensor u'input/X:0', which has shape '(?, 55)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d4e837a0ae00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         train_op, cross_entropy =             sess.run([model.train_op,\n\u001b[0;32m----> 8\u001b[0;31m                       model.cross_entropy], feed_dict=feed)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnew_confusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/acocac/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/acocac/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    945\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (50, 26, 55) for Tensor u'input/X:0', which has shape '(?, 55)'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run([model.init_op])\n",
    "\n",
    "    feed = {model.X: X, model.y: y}\n",
    "    # training step\n",
    "    for i in range(1, 30):\n",
    "        train_op, cross_entropy = \\\n",
    "            sess.run([model.train_op,\n",
    "                      model.cross_entropy], feed_dict=feed)\n",
    "\n",
    "        confusion_matrix += new_confusion_matrix\n",
    "        print(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#load pandas data (train and validation sets) - to be stored locally. Dataset available online in the repository (directory \"data\")\n",
    "in_dir = \"/home/acocac/post/tmp/\"\n",
    "df_train = pd.read_csv(in_dir + \"traindata_allclasses_byXYyear_label.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##input data preparation\n",
    "#pandas to 3d array\n",
    "train_dataset = np.array(list(df_train.groupby('Mid').apply(pd.DataFrame.as_matrix)))\n",
    "\n",
    "#settings\n",
    "n_classes = 2\n",
    "target = ['red','blue','nir','mir','fyear'] #variables\n",
    "target_label = ['0','1'] #variables \n",
    "\n",
    "index_features = map(lambda col: df_train.columns.get_loc(col), target)\n",
    "index_labels = map(lambda col: df_train.columns.get_loc(col), target_label)\n",
    "#index_labels = df_train.columns.get_loc('label')\n",
    "\n",
    "#data\n",
    "X = train_dataset[:,:,index_features]\n",
    "y = train_dataset[:,:,index_labels]\n",
    "#Y = Y.mean(axis=1, dtype='int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_length = np.random.randint(16, 23, X.shape[0])\n",
    "seq_length.fill(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "n_input = X.shape[2]\n",
    "n_classes = 2\n",
    "batch_size = y.shape[0]\n",
    "max_obs = X.shape[1]\n",
    "\n",
    "confusion_matrix = np.zeros((n_classes, n_classes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Model(n_input=n_input, max_obs=max_obs, n_classes=n_classes, n_layers=1, batch_size=batch_size, adam_lr=1e-3, dropout_keep_prob=0.5, n_cell_per_input=10, rnn_cell_type=\"lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summaryWriter = tf.summary.FileWriter(\"/home/acocac/post/tmp/log/real\", graph=tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([model.init_op])\n",
    "\n",
    "    feed = {model.X: X, model.y_: y, model.seq_lengths: seq_length}\n",
    "    # training step\n",
    "    for i in range(1, 30):\n",
    "        train_op, cross_entropy, new_confusion_matrix = \\\n",
    "            sess.run([model.train_op,\n",
    "                      model.cross_entropy,\n",
    "                      model.confusion_matrix], feed_dict=feed)\n",
    "\n",
    "        confusion_matrix += new_confusion_matrix\n",
    "        print(cross_entropy)\n",
    "\n",
    "        #a,b = eval.class_evaluation(confusion_matrix)\n",
    "        a,b = eval.calculate_accuracy_metrics(confusion_matrix)\n",
    "        scores, targets = sess.run([model.scores, tf.reshape(model.targets, [-1])], feed_dict=feed)\n",
    "        #fpr, tpr, threshold = roc_curve(targets, scores, 0)\n",
    "        #fpr, tpr, threshold = roc_curve(targets, scores, 0)\n",
    "\n",
    "        summary = sess.run(model.merge_summary_op, feed_dict=feed)\n",
    "        summaryWriter.add_summary(summary, i)\n",
    "\n",
    "    #buf = plots.plot_confusion_matrix(confusion_matrix, range(1, n_classes))\n",
    "    #image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    #image = tf.expand_dims(image, 0)\n",
    "    #summary_op = tf.image_summary(\"matplotlib conf matrix\", image)\n",
    "    #summary = sess.run(summary_op)\n",
    "    # summaryWriter.add_summary(summary, i)\n",
    "\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle as pickle\n",
    "import sklearn\n",
    " \n",
    "# http: // www.dataschool.io / simple - guide - to - confusion - matrix - terminology /\n",
    "cnf = [[50., 10.],\n",
    "       [5., 100.]]\n",
    "\n",
    "overall_accuracy, (c1, c2) = eval.calculate_accuracy_metrics(cnf)\n",
    "\n",
    "print(\"Test 1\")\n",
    "print()\n",
    "print(\"overall accurcacy correct:\", np.round(overall_accuracy, 2) == 0.91)\n",
    "print(\"c2 accuracy\", np.round(c2[\"accuracy\"], 2) == 0.91)\n",
    "print(\"c2 recall\", np.round(c2[\"recall\"], 2) == 0.95)\n",
    "print(\"c2 precision\", np.round(c2[\"precision\"], 2) == 0.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c2[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run([model.init_op])\n",
    "\n",
    "    feed = {model.X: X, model.y: y}\n",
    "    # training step\n",
    "    for i in range(1, 30):\n",
    "        train_op, cross_entropy = \\\n",
    "            sess.run([model.train_op,\n",
    "                      model.cross_entropy], feed_dict=feed)\n",
    "\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "n_input = 1 * 6 + 1\n",
    "n_classes = 2\n",
    "batch_size = 50\n",
    "max_obs = 26\n",
    "\n",
    "X = np.random.rand(batch_size, max_obs, n_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.random.rand(batch_size, max_obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = np.reshape(y, [-1, 2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train=np.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_length = np.random.randint(16, max_obs, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run([model.init_op])\n",
    "    feed = {model.y: y}\n",
    "    sess.run([model.train_op,\n",
    "                      model.cross_entropy], feed_dict=feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnf = np.array([[1971, 19, 1, 8, 0, 1],\n",
    "                    [16, 1940, 2, 23, 9, 10],\n",
    "                    [8, 3, 1891, 87, 0, 11],\n",
    "                    [2, 25, 159, 1786, 16, 12],\n",
    "                    [0, 24, 4, 8, 1958, 6],\n",
    "                    [11, 12, 29, 11, 11, 1926]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overall_accuracy, (c1, c2, c3, c4, c5, c6) = eval.calculate_accuracy_metrics(cnf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"c1 accuracy\", np.round(c1[\"accuracy\"], 4) == 0.9943)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
