{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "n_input = 9 * 6 + 1 #[n_pixels * n_bands + n_time]\n",
    "n_classes = 2\n",
    "batch_size = 50\n",
    "max_obs = 26\n",
    "\n",
    "X = np.random.rand(batch_size, max_obs, n_input)\n",
    "y = np.random.rand(batch_size, max_obs, n_classes)\n",
    "\n",
    "seq_length = np.random.randint(16, max_obs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow - Paper approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn as rnn_cell\n",
    "import io\n",
    "from util.tf_utils import tf_confusion_metrics\n",
    "import inspect\n",
    "import util.eval as eval\n",
    "import util.plots as plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \"\"\"\n",
    "    Tensorflow Graph using Recurrent LSTM layers and fully connected softmax layer for field identification\n",
    "    with multispectral/temporal data acquired from satellite imagery\n",
    "    Params\n",
    "        tf placeholders:\n",
    "            X           Input data cube of dimensions [batch_size x max_observations x n_input]\n",
    "            y           Target data Tensor of dimensions [batch_size x max_observations]\n",
    "            seq_lenghts Number of observations for each batch if observation < max_obs data is\n",
    "                        padded with zeros [batch_size]\n",
    "        input parameters:\n",
    "            n_input     length of observed pixel values. [n_pixels * n_bands + n_time]\n",
    "                n_pixels    number of observed pixels (default 3*3)\n",
    "                n_bands     number of observed bands  (default 6)\n",
    "                n_time      number of time parameters (default 1 e.g. day of year)\n",
    "            n_classes   number of target classes\n",
    "            batch_size  number of batches\n",
    "            max_obs     maximum number of observations if seq_lengs < max_obs matrices will be padded\n",
    "                        controls number of iterations in rnn layers (aka sequence length)\n",
    "        network specific parameters\n",
    "            n_layers    number of rnn layers (aka depth)\n",
    "            learning_rate\n",
    "            dropout_keep_prob\n",
    "            logdir\n",
    "    Marc.Russwurm@tum.de\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_input=9 * 6 + 1, n_classes=20, batch_size=50, max_obs=26,\n",
    "                 n_layers=2, dropout_keep_prob=.5, adam_lr=1e-3, adam_b1=0.9, adam_b2=0.999, adam_eps=1e-8,\n",
    "                 fc_w_stddev=0.1, fc_b_offset=0.1, n_cell_per_input=1,rnn_cell_type=\"basiclstm\", gpu=None):\n",
    "        # save input arguments\n",
    "        self.args = inspect.getargvalues(inspect.currentframe()).locals\n",
    "        del self.args[\"self\"]  # delete self argument\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        with tf.device(None):\n",
    "\n",
    "            with tf.variable_scope('input'):\n",
    "                # block of [batch_size x max_obs x n_input]\n",
    "                self.X = tf.placeholder(tf.float32, [batch_size, max_obs, n_input], name=\"X\")\n",
    "                self.y_ = self.y = y_ = tf.placeholder(tf.float32, [batch_size, max_obs, n_classes], name=\"y\")\n",
    "                self.seq_lengths = seq_lengths = tf.placeholder(tf.int32, [batch_size], name=\"seq_lengths\")\n",
    "                #self.y = y = tf.reshape(self.y_, [-1, n_classes], name=\"y\")\n",
    "\n",
    "            with tf.name_scope('RNN'):\n",
    "                self.n_rnn_cells = n_rnn_cells = n_cell_per_input * n_input\n",
    "\n",
    "                if rnn_cell_type == \"basiclstm\":\n",
    "                    cell = rnn_cell.BasicLSTMCell(n_rnn_cells)\n",
    "                if rnn_cell_type == \"lstm\":\n",
    "                    cell = rnn_cell.LSTMCell(n_rnn_cells)\n",
    "                if rnn_cell_type == \"lstm_peephole\":\n",
    "                    cell = rnn_cell.LSTMCell(n_rnn_cells, use_peepholes=True)\n",
    "                elif rnn_cell_type == \"gru\":\n",
    "                    cell = rnn_cell.BasicLSTMCell(n_rnn_cells)\n",
    "                elif rnn_cell_type == \"rnn\":\n",
    "                    cell = rnn_cell.BasicRNNCell(n_rnn_cells)\n",
    "\n",
    "                # dropout Wrapper\n",
    "                cell = tf.contrib.rnn.DropoutWrapper(cell=cell, output_keep_prob=dropout_keep_prob)\n",
    "                self.cell = cell = rnn_cell.MultiRNNCell([cell] * n_layers)\n",
    "\n",
    "                # tensor with class labels of dimension [batch_size x max_obs]\n",
    "                # defined as Variable to carry values to next iteration (not trainable must be declared explicitly)\n",
    "                self.state = state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "                # rnn_outputs: block of [batch_size x max_obs x rnn_size]\n",
    "                # data is padded with zeros after seq_length\n",
    "                outputs, last_states = tf.nn.dynamic_rnn(cell, self.X, initial_state=state, sequence_length=seq_lengths,\n",
    "                                                         time_major=False)\n",
    "\n",
    "                self.outputs = outputs\n",
    "                self.last_states = last_states\n",
    "\n",
    "            with tf.name_scope('fc'):\n",
    "                # reshape outputs to: block of [batch_size * max_obs x rnn_size]\n",
    "                softmax_in = tf.reshape(outputs, [-1, n_rnn_cells])\n",
    "                softmax_w = tf.Variable(tf.truncated_normal([n_rnn_cells, n_classes], stddev=fc_w_stddev), name=\"W_softmax\")\n",
    "                softmax_b = tf.Variable(tf.constant(fc_b_offset, shape=[n_classes]), name=\"b_softmax\")\n",
    "\n",
    "                softmax_out = tf.matmul(softmax_in, softmax_w) + softmax_b\n",
    "                self.logits = logits = tf.reshape(softmax_out, [batch_size, -1, n_classes])\n",
    "\n",
    "            with tf.name_scope('train'):\n",
    "                # Define loss and optimizer\n",
    "\n",
    "                # create mask for cross entropies incases where seq_lengths < max_max_obs\n",
    "                # masking from http://stackoverflow.com/questions/34128104/tensorflow-creating-mask-of-varied-lengths\n",
    "\n",
    "                with tf.name_scope('mask'):\n",
    "                    lengths_transposed = tf.expand_dims(seq_lengths, 1)\n",
    "\n",
    "                    range = tf.range(0, max_obs, 1)\n",
    "                    range_row = tf.expand_dims(range, 0)\n",
    "\n",
    "                    self.mask = mask = tf.less(range_row, lengths_transposed)\n",
    "\n",
    "                self.cross_entropy_matrix = cross_entropy_matrix = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_)\n",
    "                self.masked_cross_entropy_matrix = masked_cross_entropy_matrix = tf.where(mask, cross_entropy_matrix,\n",
    "                                                                                           tf.zeros(mask.get_shape()))\n",
    "                self.cross_entropy_matrix = cross_entropy_matrix = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_)\n",
    "\n",
    "                # normalize with total number of observations\n",
    "                self.cross_entropy = cross_entropy = tf.reduce_sum(cross_entropy_matrix) / tf.cast(\n",
    "                    tf.reduce_sum(seq_lengths), tf.float32)\n",
    "                tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "                # grad_train_op = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "                self.train_op = tf.train.AdamOptimizer(learning_rate=adam_lr, beta1=adam_b1, beta2=adam_b2,\n",
    "                                                       epsilon=adam_eps).minimize(cross_entropy)\n",
    "                # tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "            with tf.name_scope('evaluation'):\n",
    "\n",
    "                self.probabilities = probs = tf.nn.softmax(logits, name=\"full_probability_matrix\")\n",
    "\n",
    "                # Evaluate model\n",
    "                predicted = tf.argmax(logits, 2)\n",
    "                targets = tf.argmax(y_, 2)\n",
    "\n",
    "                correct_pred = tf.equal(predicted, targets)\n",
    "                masked_correct_pred = tf.logical_and(mask, correct_pred)\n",
    "                self.accuracy_op = accuracy = tf.reduce_sum(tf.cast(masked_correct_pred, tf.float32)) / tf.cast(\n",
    "                    tf.reduce_sum(seq_lengths), tf.float32)\n",
    "                tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "                self.probs_list = probs_list = tf.reshape(probs, (-1, n_classes))\n",
    "                predicted_list = tf.reshape(predicted, [-1])\n",
    "                targets_list = tf.reshape(targets, [-1])\n",
    "\n",
    "                mask_list = tf.reshape(mask, [-1])\n",
    "\n",
    "                one_hot_targets = tf.one_hot(targets_list, n_classes)\n",
    "                scores = tf.boolean_mask(probs_list, tf.cast(one_hot_targets, tf.bool))\n",
    "\n",
    "                # mask of individual number of observations\n",
    "                obs_list = tf.tile(tf.range(0, max_obs), [batch_size])\n",
    "                obs_matrix = tf.matmul(tf.expand_dims(obs_list, 1), tf.ones([1, n_classes], dtype=tf.int32))\n",
    "\n",
    "                probs_matrix_mask = probs_matrix_mask = tf.transpose(tf.reshape(tf.tile(mask_list, [n_classes]),[n_classes,-1]))\n",
    "\n",
    "                self.scores = tf.boolean_mask(probs_list, probs_matrix_mask)\n",
    "                self.targets = tf.boolean_mask(tf.reshape(y_, [-1,n_classes]), probs_matrix_mask)\n",
    "                self.obs = tf.boolean_mask(obs_list, mask_list)\n",
    "\n",
    "                # drop all values which are > seqlength\n",
    "                #self.scores = tf.boolean_mask(scores, mask_list)\n",
    "                #self.targets = tf.boolean_mask(targets_list, mask_list)\n",
    "                #self.obs = tf.boolean_mask(obs_list, mask_list)\n",
    "\n",
    "                self.confusion_matrix = confusion_matrix = tf.contrib.metrics.confusion_matrix(\n",
    "                    tf.boolean_mask(targets_list, mask_list),\n",
    "                    tf.boolean_mask(predicted_list, mask_list),\n",
    "                    num_classes=n_classes)\n",
    "\n",
    "                confusion_matrix = tf.cast(confusion_matrix, tf.uint8)\n",
    "                confusion_matrix = tf.expand_dims(confusion_matrix, 2)\n",
    "                confusion_matrix = tf.expand_dims(confusion_matrix, 0)\n",
    "                tf.summary.image(\"confusion_matrix\", confusion_matrix, max_outputs=3)\n",
    "\n",
    "                logits_ = tf.cast(logits, tf.uint8)\n",
    "                logits_ = tf.expand_dims(logits_, 3)\n",
    "                tf.summary.image(\"logits\", logits_, max_outputs=1)\n",
    "\n",
    "                probs_ = tf.cast(probs*255, tf.uint8)\n",
    "                probs_ = tf.expand_dims(probs_, 3)\n",
    "                tf.summary.image(\"probabilities\", probs_, max_outputs=1)\n",
    "\n",
    "                targets_ = tf.cast(y_, tf.uint8)\n",
    "                targets_ = tf.expand_dims(targets_, 3)\n",
    "                tf.summary.image(\"targets\", targets_, max_outputs=1)\n",
    "\n",
    "                # tf.add_to_collection(tf.GraphKeys.SUMMARIES, cm_im_summary)\n",
    "\n",
    "            # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "            self.merge_summary_op = tf.summary.merge_all()\n",
    "            self.init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unroll(x, y, seq_lengths):\n",
    "    \"\"\"\n",
    "        Reshapes and masks input and output data from\n",
    "        X(batchsize x n_max_obs x n_input) -> X_ (new_batchsize x n_input)\n",
    "        y(batchsize x n_max_obs x n_classes) -> X_ (new_batchsize x n_classes)\n",
    "        new_batch_size is variable representing batchsize * n_max_obs - invalid_observations\n",
    "        with invalid observations being observations > seq_length -> means\n",
    "        if at one point only 24 of maximum 26 images are available X is usually padded with zeros\n",
    "        this masking removes the last two observations\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # create mask for valid times of acquisition\n",
    "    batch_size, max_seqlengths, n_input = x.shape\n",
    "    np.arange(0, max_seqlengths) * np.ones((batch_size, max_seqlengths))\n",
    "    ones = np.ones([batch_size, max_seqlengths])\n",
    "    mask = np.arange(0, max_seqlengths) * ones < (seq_lengths * ones.T).T\n",
    "\n",
    "    new_x = x[mask]\n",
    "    new_y = y[mask]\n",
    "\n",
    "    return new_x, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1 LSTM layer with 110 cells (neurons) / neurons (n_input x n_cell_per_input)\n",
    "model = Model(n_input=n_input, n_classes=n_classes, n_layers=1, batch_size=batch_size, adam_lr=1e-3, dropout_keep_prob=0.5, n_cell_per_input=2, rnn_cell_type = \"lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.862016\n",
      "0.875358\n",
      "0.860546\n",
      "0.86441\n",
      "0.866694\n",
      "0.863071\n",
      "0.859572\n",
      "0.855336\n",
      "0.856485\n",
      "0.859536\n",
      "0.858298\n",
      "0.857028\n",
      "0.854954\n",
      "0.856428\n",
      "0.858219\n",
      "0.856485\n",
      "0.857076\n",
      "0.856296\n",
      "0.854567\n",
      "0.856852\n",
      "0.857041\n",
      "0.856983\n",
      "0.857584\n",
      "0.85522\n",
      "0.85648\n",
      "0.858433\n",
      "0.855867\n",
      "0.857744\n",
      "0.857313\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#run and write results\n",
    "\n",
    "confusion_matrix = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "summaryWriter = tf.summary.FileWriter(\"/home/acocac/post/tmp/log/synthetic\", graph=tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([model.init_op])\n",
    "\n",
    "    feed = {model.X: X, model.y_: y, model.seq_lengths: seq_length}\n",
    "    \n",
    "    # training step\n",
    "    for i in range(1, 30):\n",
    "        train_op, cross_entropy, new_confusion_matrix = \\\n",
    "            sess.run([model.train_op,\n",
    "                      model.cross_entropy,\n",
    "                      model.confusion_matrix], feed_dict=feed)\n",
    "\n",
    "        confusion_matrix += new_confusion_matrix\n",
    "        print(cross_entropy)\n",
    "\n",
    "        a,b = eval.calculate_accuracy_metrics(confusion_matrix)\n",
    "        scores, targets = sess.run([model.scores, tf.reshape(model.targets, [-1])], feed_dict=feed)\n",
    "        \n",
    "        summary = sess.run(model.merge_summary_op, feed_dict=feed)\n",
    "        summaryWriter.add_summary(summary, i)\n",
    "\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51311807934\n"
     ]
    }
   ],
   "source": [
    "#computing performance metrix from confusion matrix\n",
    "overall_accuracy, (c1, c2) = eval.calculate_accuracy_metrics(confusion_matrix)\n",
    "print(overall_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano KERAS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#functions\n",
    "def onehot(labels):\n",
    "    Uniques,Index  = np.unique(labels,return_inverse=True)\n",
    "    return np_utils.to_categorical(Index,len(Uniques))\n",
    "\n",
    "def inverse_onehot(matrix):\n",
    "    labels =[]\n",
    "    for row in matrix:\n",
    "        labels.append(np.argmax(row,axis=1))\n",
    "    return labels   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reshape label array\n",
    "u = y.mean(axis=1,  keepdims=True) #mean probabilities \n",
    "a = inverse_onehot(u) #determine label\n",
    "Y = np.array(a) \n",
    "Y = Y.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LSTMRNN = {\"name\": 'Long Short Term Memory (LSTM)', \"results\" : [],\"time\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "#from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68301576376\n",
      "0.668826520443\n"
     ]
    }
   ],
   "source": [
    "#fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "#define k-fold cross validation\n",
    "folds = 2\n",
    "kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed) #ensure each fold is different and each sample is seen only once.\n",
    "\n",
    "#LSTM and FFNN model settings\n",
    "batch_size = 50 \n",
    "n_epoch = 20 \n",
    "\n",
    "#empty df\n",
    "res = pd.DataFrame(columns=['acc', 'acc_std']) #to store results\n",
    "\n",
    "for train, test in kfold.split(X, Y): \n",
    "    \n",
    "    #input data LSTM\n",
    "    train_data = X[train]\n",
    "    test_data = X[test]\n",
    "\n",
    "    model = Sequential()  \n",
    "    model.add(LSTM(50, input_shape=(train_data.shape[1], train_data.shape[2])))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8), metrics=['acc'])\n",
    "\n",
    "    model.fit(train_data, onehot(Y[train]), batch_size=batch_size, nb_epoch=n_epoch, verbose=0)\n",
    "\n",
    "    scores = model.evaluate(test_data, onehot(Y[test]), verbose=0, batch_size=1000)\n",
    "    print scores[0]\n",
    "    \n",
    "    LSTMRNN['results'].append(scores[1] * 100)\n",
    "    res.loc[LSTMRNN['name'], 'acc'] = np.mean(LSTMRNN['results'])\n",
    "    res.loc[LSTMRNN['name'], 'acc_std'] = np.std(LSTMRNN['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Long Short Term Memory (LSTM)</th>\n",
       "      <td>54</td>\n",
       "      <td>3.4641</td>\n",
       "      <td>21302.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              acc acc_std  parameters\n",
       "Long Short Term Memory (LSTM)  54  3.4641     21302.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
